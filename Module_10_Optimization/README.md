# Optimization

## Topics covered in today's module
* Gradient Descent
* Stochastic Gradient Descent
* Mini-Batch Gradient Descent
* Hyperparameter Tuning

## Main takeaways from doing today's assignment
I reviewed the differences between gradient descent, stochastic gradient descent, and mini-batch stochastic gradient descent, as well as their advantages and disadvantages. I compared the loss values of stochastic gradient descent (with and without momentum, with and without weight decay), AdaGrad, RMSProp, and Adam.

## Challenging, interesting, or exciting aspects of today's assignment
It was interesting to see the differences between the loss values of the various optimization algorithms.

## Additional resources
1. Did you need to use tools like AI tools ChatGPT or Gemini to answer any of the questions or learn any of the concepts in this notebook? If  yes, state for which questions or concepts did you require Generative AI tools? 

    I did not use such tools.

2. If you answered "yes" to Question #1, which part of the answers are written by you, and which part of the answers are written by an AI tool? 

    Not applicable.

3. If you answered "yes" to Question #1, after using the AI tool to gain a better understanding of the material in this notebook, summarize your learnings here.

    Not applicable.

4. Did you use any other resources besides AI tools?

    No.
