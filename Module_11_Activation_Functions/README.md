# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment
I learned about the advantages and disadvantages of several activation functions: sigmoid, tanh, ReLU, leaky ReLU, and ELU. I was introduced to a few advanced activation functions: swish, GeLU, and SeLU. I learned about how ReLU avoids the vanishing gradient problem, how leaky ReLU solves the dying ReLU problem, and how ELU is more robust to noise than leaky ReLU.

## Challenging, interesting, or exciting aspects of today's assignment
It was interesting to learn about how activation functions build on each other's issues and solve well-known problems.

## Additional resources
1. Did you need to use tools like AI tools ChatGPT or Gemini to answer any of the questions or learn any of the concepts in this notebook? If  yes, state for which questions or concepts did you require Generative AI tools? 

    I did not use such tools.

2. If you answered "yes" to Question #1, which part of the answers are written by you, and which part of the answers are written by an AI tool? 

    Not applicable.

3. If you answered "yes" to Question #1, after using the AI tool to gain a better understanding of the material in this notebook, summarize your learnings here.

    Not applicable.

4. Did you use any other resources besides AI tools?

    No.
